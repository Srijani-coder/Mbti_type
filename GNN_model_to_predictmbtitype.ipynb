{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CAOQT3y9fSd",
        "outputId": "fc65bec0-3d65-44a0-987c-661b209cce6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.12.3\n",
            "Mounted at /content/drive\n",
            "the type wise distributionof data: \n",
            "\n",
            "type\n",
            "INTP    24961\n",
            "INTJ    22427\n",
            "INFJ    14963\n",
            "INFP    12134\n",
            "ENTP    11725\n",
            "ENFP     6167\n",
            "ISTP     3424\n",
            "ENTJ     2955\n",
            "ESTP     1986\n",
            "ENFJ     1534\n",
            "ISTJ     1243\n",
            "ISFP      875\n",
            "ISFJ      650\n",
            "ESTJ      482\n",
            "ESFP      360\n",
            "ESFJ      181\n",
            "Name: count, dtype: int64\n",
            "type\n",
            "ENTP    6629\n",
            "INTP    6629\n",
            "ESTJ    6629\n",
            "ESTP    6629\n",
            "ENTJ    6629\n",
            "ESFJ    6629\n",
            "ISFJ    6629\n",
            "ENFJ    6629\n",
            "INFJ    6629\n",
            "INFP    6629\n",
            "ISFP    6629\n",
            "ESFP    6629\n",
            "ENFP    6629\n",
            "ISTJ    6629\n",
            "INTJ    6629\n",
            "ISTP    6629\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "!pip install imbalanced-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "encode = LabelEncoder()\n",
        "\n",
        "# the MBTI 500 dataset path in my drive folder\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset_DSAPP/MBTI 500.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dA8Ol9BW9jFv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculating average number of words in post\n",
        "df['wordsNoAvg'] = df['posts'].apply(lambda x: np.mean([len(item.split()) for item in x.split('|||')]))\n",
        "\n",
        "def build_graph_with_extracted_features(df, mbtitype):\n",
        "    # dataframe for each MBTI type\n",
        "    df_part = df[df['type'] == mbtitype]\n",
        "\n",
        "\n",
        "    #  TF-IDF matrix features from text in posts\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    tfidf = vectorizer.fit_transform(df_part['posts'])\n",
        "\n",
        "    # cosine similarity matrix calculation from tfidf\n",
        "    cos = cosine_similarity(tfidf)\n",
        "\n",
        "    # preparing average no of words in post feature\n",
        "    features = np.hstack((tfidf.toarray(), df_part[['wordsNoAvg']].values))\n",
        "\n",
        "    # node features tensor\n",
        "    nodeOfeatures = torch.tensor(features, dtype=torch.float)\n",
        "    labels = torch.tensor(df_part['numeric_type'].values, dtype=torch.long)\n",
        "\n",
        "    # Creating edge indices based on cosine similarity (threshold for similarity)\n",
        "    index_for_edge= []\n",
        "    thresh = 0.5  # experimental threshold\n",
        "    for i in range(len(df_part)):\n",
        "        for j in range(i + 1, len(df_part)):\n",
        "            if cos[i, j] > thresh:\n",
        "                index_for_edge.append([i, j])\n",
        "                index_for_edge.append([j, i])\n",
        "    index_for_edge = torch.tensor(index_for_edge, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    return Data(x=nodeOfeatures, edge_index=index_for_edge, y=labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP1JJ4DOIaWG",
        "outputId": "dd1cce06-af52-40ec-df0e-dbc7a419a9c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/64.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m51.2/64.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m633.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Collecting aiohttp (from torch_geometric)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->torch_geometric)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch_geometric)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch_geometric\n",
            "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 torch_geometric-2.5.3 yarl-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "# Converting PyTorch Geometric data to NetworkX graph for visualization\n",
        "def drawing_graph(data, title):\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    p = nx.spring_layout(G, seed=42)\n",
        "    nx.draw(G, p, node_size=50, node_color=data.y.numpy(), cmap=plt.get_cmap('Set1'), with_labels=False)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "juMhe-YyHkjp"
      },
      "outputs": [],
      "source": [
        "# For each type , the graph gets stored in dictionary\n",
        "# key of dictionary: type\n",
        "# value of dictionary : graph\n",
        "# Visualizing each graph\n",
        "mbs = df['type'].unique()\n",
        "graphs = {}\n",
        "\n",
        "for mb in mbs:\n",
        "    graphs[mb] = build_graph_with_extracted_features(df, mb)\n",
        "    title = str(mb) + '_Graph'\n",
        "    drawing_graph(graphs[mb], title)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-MR8KspDHnT6"
      },
      "outputs": [],
      "source": [
        "# Shiuffling and combining graphs  to construct graph dataset\n",
        "\n",
        "def combine_graphs(graphs):\n",
        "    nodes = []\n",
        "    edges = []\n",
        "    labels = []\n",
        "\n",
        "    offset = 0\n",
        "    for graph in graphs.values():\n",
        "        num_nodes = graph.x.size(0) # number of nodes\n",
        "\n",
        "        # Adjusting edge indices to account for combined node indexing\n",
        "        edge = graph.edge_index + offset\n",
        "        edges.append(edge)\n",
        "\n",
        "        nodes.append(graph.x)\n",
        "        labels.append(graph.y)\n",
        "\n",
        "        offset += num_nodes\n",
        "\n",
        "    combnodes = torch.cat(nodes, dim=0)\n",
        "    combedges = torch.cat(edges, dim=1)\n",
        "    comblabels = torch.cat(labels, dim=0)\n",
        "\n",
        "    return Data(x=combnodes, edge_index=combedges, y=comblabels)\n",
        "\n",
        "combgraph = combine_graphs(graphs)\n",
        "\n",
        "# permuting the combined graph data to ensure shuffling of data\n",
        "num_nodes = combgraph.x.size(0)\n",
        "permute = torch.randperm(num_nodes)\n",
        "shuffnodes = combgraph.x[permute]\n",
        "shufflabels = combgraph.y[permute]\n",
        "shuff_edge_index = combgraph.edge_index\n",
        "\n",
        "# Turning into final shuffled graph\n",
        "shuffgraph = Data(x=shuffnodes, edge_index=shuff_edge_index, y=shufflabels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n_4FPzh2Hxev"
      },
      "outputs": [],
      "source": [
        "# data split\n",
        "# Training dataset size: 60%\n",
        "# Validation dataset size: 20%\n",
        "# Test dataset size: 20%\n",
        "def splittingdata(graph, train_ratio=0.6, val_ratio=0.2):\n",
        "    num_nodes = graph.x.size(0)\n",
        "    tr_size = int(train_ratio * num_nodes)\n",
        "    v_size = int(val_ratio * num_nodes)\n",
        "    te_size = num_nodes - train_size - val_size\n",
        "\n",
        "    tr_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    v_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    te_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    tr_mask[:tr_size] = True\n",
        "    v_mask[tr_size:tr_size + v_size] = True\n",
        "    te_mask[tr_size + v_size:] = True\n",
        "\n",
        "    return tr_mask, v_mask, te_mask\n",
        "\n",
        "train_mask, val_mask, test_mask = splittingdata(shuffgraph)\n",
        "\n",
        "# Assigning masks to the graph data\n",
        "shuffgraph.train_mask = train_mask\n",
        "shuffgraph.val_mask = val_mask\n",
        "shuffgraph.test_mask = test_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WcV855kHH6_E"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx, to_networkx\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "import torch.nn as nn\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, log_loss, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T2Cl6_Q98KrF"
      },
      "outputs": [],
      "source": [
        "# Defining the GATClassifier\n",
        "class GATClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n",
        "        super(GATClassifier, self).__init__()\n",
        "        self.conv1 = GATConv(in_dim, hidden_dim, heads=num_heads, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=False, dropout=0.6)\n",
        "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initializing the model, optimizer, and loss function\n",
        "in_dim = shuffgraph.x.shape[1]\n",
        "hidden_dim = 64\n",
        "num_heads = 8\n",
        "n_classes = len(encode.classes_)\n",
        "model = GATClassifier(in_dim, hidden_dim, num_heads, n_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, optimizer, criterion, data, train_mask):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[train_mask], data.y[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate(model, criterion, data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        loss = criterion(out[mask], data.y[mask]).item()\n",
        "        pred = out[mask].max(dim=1)[1]\n",
        "        correct = pred.eq(data.y[mask]).sum().item()\n",
        "        accuracy = correct / mask.sum().item()\n",
        "    return loss, accuracy,out[mask]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XwnduFoJbSa",
        "outputId": "f685828c-ab02-4362-8074-188f9dd7a4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000: Train Loss: 1.9865, Val Loss: 0.9300, Val Accuracy: 0.8666\n",
            "Epoch 010: Train Loss: 2.0882, Val Loss: 0.9983, Val Accuracy: 0.8567\n",
            "Epoch 020: Train Loss: 2.0439, Val Loss: 0.9899, Val Accuracy: 0.8628\n",
            "Epoch 030: Train Loss: 2.4946, Val Loss: 1.4367, Val Accuracy: 0.6870\n",
            "Epoch 040: Train Loss: 5.5075, Val Loss: 5.3316, Val Accuracy: 0.2466\n",
            "Epoch 050: Train Loss: 5.2636, Val Loss: 3.0389, Val Accuracy: 0.4789\n",
            "Epoch 060: Train Loss: 2.8756, Val Loss: 1.3957, Val Accuracy: 0.7750\n",
            "Epoch 070: Train Loss: 2.1538, Val Loss: 0.8971, Val Accuracy: 0.8756\n",
            "Epoch 080: Train Loss: 1.8941, Val Loss: 0.6708, Val Accuracy: 0.8995\n",
            "Epoch 090: Train Loss: 1.7793, Val Loss: 0.7546, Val Accuracy: 0.8785\n",
            "Test Loss: 0.7280, Test Accuracy: 0.8689\n",
            "Test F1 Score: 0.8278\n",
            "Test Precision: 0.8349\n",
            "Test Recall: 0.8689\n",
            "Test MCC: 0.8454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/Dataset_DSAPP/gat_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Training and validation\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, optimizer, criterion, shuffgraph, shuffgraph.train_mask)\n",
        "    val_loss, val_accuracy, _ = evaluate(model, criterion, shuffgraph, shuffgraph.val_mask)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_accuracy, test_logits = evaluate(model, criterion, shuffgraph, shuffgraph.test_mask)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Calculating F1 score\n",
        "test_pred = test_logits.max(dim=1)[1]\n",
        "test_f1 = f1_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Calculating Precision\n",
        "test_precision = precision_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "\n",
        "# Calculating Recall\n",
        "test_recall = recall_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "\n",
        "# Calculaing Matthews Correlation Coefficient\n",
        "test_mcc = matthews_corrcoef(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu())\n",
        "print(f\"Test MCC: {test_mcc:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model_path = '/content/drive/MyDrive/Dataset_DSAPP/gat_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mD_73-TFQ0HD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim = 16):\n",
        "        super(GCPModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XrkwKhV8Q2Ez"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, optimizer, criterion, data, mask):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output[mask], data.y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, criterion, data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(data)\n",
        "        loss = criterion(output[mask], data.y[mask])\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct = pred[mask].eq(data.y[mask]).sum().item()\n",
        "        accuracy = correct / mask.sum().item()\n",
        "    return loss, accuracy, output[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ObpjUAHaHAY",
        "outputId": "73641f20-5cb9-4bd6-a69a-b45fab7bc961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 000: Train Loss: 11.3003, Val Loss: 5.9652, Val Accuracy: 0.2062\n",
            "Epoch 010: Train Loss: 2.4495, Val Loss: 2.2992, Val Accuracy: 0.2062\n",
            "Epoch 020: Train Loss: 2.2508, Val Loss: 2.1538, Val Accuracy: 0.2391\n",
            "Epoch 030: Train Loss: 2.1879, Val Loss: 2.1049, Val Accuracy: 0.2394\n",
            "Epoch 040: Train Loss: 2.1434, Val Loss: 2.0602, Val Accuracy: 0.2066\n",
            "Epoch 050: Train Loss: 2.0824, Val Loss: 1.9907, Val Accuracy: 0.3620\n",
            "Epoch 060: Train Loss: 2.0030, Val Loss: 1.8823, Val Accuracy: 0.4861\n",
            "Epoch 070: Train Loss: 1.9095, Val Loss: 1.7594, Val Accuracy: 0.4767\n",
            "Epoch 080: Train Loss: 1.8628, Val Loss: 1.6569, Val Accuracy: 0.4296\n",
            "Epoch 090: Train Loss: 1.7510, Val Loss: 1.4869, Val Accuracy: 0.5766\n",
            "Test Loss: 1.5122, Test Accuracy: 0.6480\n",
            "Test F1 Score: 0.5574\n",
            "Test Precision: 0.5103\n",
            "Test Recall: 0.6480\n",
            "Test MCC: 0.5942\n",
            "Model saved to /content/drive/MyDrive/Dataset_DSAPP/gcn_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Initializing the model, optimizer, and criterion\n",
        "input_dim = shuffgraph.num_node_features\n",
        "hidden_dim = 64\n",
        "model = GCPModel(input_dim, hidden_dim, output_dim = 16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and validation loop\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, optimizer, criterion, shuffgraph, shuffgraph.train_mask)\n",
        "    val_loss, val_accuracy, _ = evaluate(model, criterion, shuffgraph, shuffgraph.val_mask)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluating on the test set\n",
        "test_loss, test_accuracy, test_logits = evaluate(model, criterion, shuffgraph, shuffgraph.test_mask)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Calculating evaluation metrics\n",
        "test_pred = test_logits.argmax(dim=1)\n",
        "\n",
        "# Calculating F1 score\n",
        "test_f1 = f1_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# Calculating Precision\n",
        "test_precision = precision_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "\n",
        "# Calculating Recall\n",
        "test_recall = recall_score(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu(), average='weighted')\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "\n",
        "# Calculating Matthews Correlation Coefficient\n",
        "test_mcc = matthews_corrcoef(shuffgraph.y[shuffgraph.test_mask].cpu(), test_pred.cpu())\n",
        "print(f\"Test MCC: {test_mcc:.4f}\")\n",
        "\n",
        "# Saveing the model\n",
        "model_path = '/content/drive/MyDrive/Dataset_DSAPP/gcn_model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
